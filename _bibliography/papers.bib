---
---


@inproceedings{llm_survey24,
author = {Guangji Bai and  Zheng Chai and Chen Ling and Shiyu Wang and Jiaying Lu and Nan Zhang and Tingwei Shi and Ziyang Yu and Mengdan Zhu and Yifei Zhang and Carl Yang and Yue Cheng and Liang Zhao},
title = {Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models},
year = {2024},
abbr = {arXiv},
arxiv={https://arxiv.org/abs/2401.00625},
code = {https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers},
selected={true},
}

@inproceedings{lambdaFS_asplos23,
author = {Benjamin Carver and Runzhou Han and Jingyuan Zhang and Mai Zheng and Yue Cheng},
title = {λFS: A Scalable and Elastic Distributed File System Metadata Service using Serverless Functions},
year = {2023},
booktitle = {28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
abbr = {ASPLOS'23},
arxiv={https://arxiv.org/abs/2306.11877},
code = {https://github.com/ds2-lab/LambdaFS},
selected={true},
}

@inproceedings{infinistore_vldb23,
author = {Jingyuan Zhang and Ao Wang and Xiaolong Ma and Benjamin Carver and Nicholas John Newman and Ali Anwar and Lukas Rupprecht and Dimitrios Skourtis and Vasily Tarasov and Feng Yan and Yue Cheng},
title = {InfiniStore: Elastic Serverless Cloud Storage},
year = {2023},
booktitle = {49th International Conference on Very Large Data Bases},
abbr = {VLDB'23},
arxiv={https://arxiv.org/abs/2209.01496},
pdf = {vldb23-infinistore.pdf},
code={https://github.com/ds2-lab/infinistore},
selected={true},
}

@inproceedings{shade_fast23,
author = {Redwan Ibne Seraj Khan and Ahmad Hossein Yazdani and Yuqi Fu and Arnab K. Paul and Bo Ji and Xun Jian and Yue Cheng and Ali R. Butt},
title = {SHADE: Enable Fundamental Cacheability for Distributed Deep Learning Training},
booktitle = {21th USENIX Conference on File and Storage Technologies (FAST 23)},
year = {2023},
month = feb,
abbr = {USENIX FAST'23},
talk = {https://www.usenix.org/conference/fast23/presentation/khan},
code = {https://github.com/R-I-S-Khan/SHADE},
selected={true},
pdf={fast23-shade.pdf},
}

@inproceedings{lossy_drbsd8,
author = {Zhaoyuan Su and Sheng Di and Ali Murat Gok and Yue Cheng and Franck Cappello},
title = {Understanding Impact of Lossy Compression on Derivative-related Metrics in Scientific Datasets},
year = {2022},
booktitle = {Proceedings of the 8th International Workshop on Data Analysis and Reduction for Big Scientific Data},
series = {DRBSD-8 '22},
abbr = {DRBSD-8 '22},
}

@inproceedings{sion_preprint,
author = {Jingyuan Zhang and Ao Wang and Xiaolong Ma and Benjamin Carver and Nicholas John Newman and Ali Anwar and Lukas Rupprecht and Dimitrios Skourtis and Vasily Tarasov and Feng Yan and Yue Cheng},
title = {InfiniStore: Elastic Serverless Cloud Storage},
year = {2022},
booktitle = {Preprint},
abbr = {Preprint},
arxiv={https://arxiv.org/abs/2209.01496},
code={https://github.com/ds2-lab/infinistore},
}

@inproceedings{sfs_sc22,
author = {Yuqi Fu and Li Liu and Haoliang Wang and Yue Cheng and Songqing Chen},
title = {SFS: Smart OS Scheduling for Serverless Functions},
year = {2022},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
series = {SC '22},
abbr = {SC'22},
selected={true},
arxiv={https://arxiv.org/abs/2209.01709},
pdf = {sc22-sfs-ae.pdf},
code={https://github.com/ds2-lab/SFS},
award = {<strong>Best Student Paper Award Finalist</strong>},
}

@inproceedings{rkube_socc21,
abbr={SoCC'21},
author = {Liu, Li and Wang, Haoliang and Wang, An and Xiao, Mengbai and Cheng, Yue and Chen, Songqing},
title = {Mind the Gap: Broken Promises of CPU Reservations in Containerized Multi-Tenant Clouds},
year = {2021},
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472883.3486997},
doi = {10.1145/3472883.3486997},
abstract = {Containerization is becoming increasingly popular, but unfortunately, containers often fail to deliver the anticipated performance with the allocated resources. In this paper, we first demonstrate the performance variance and degradation are significant (by up to 5x) in a multi-tenant environment where containers are co-located. We then investigate the root cause of such performance degradation. Contrary to the common belief that such degradation is caused by resource contention and interference, we find that there is a gap between the amount of CPU a container reserves and actually gets. The root cause lies in the design choices of today's Linux scheduling mechanism, which we call Forced Runqueue Sharing and Phantom CPU Time. In fact, there are fundamental conflicts between the need to reserve CPU resources and Completely Fair Scheduler's work-conserving nature, and this contradiction prevents a container from fully utilizing its requested CPU resources. As a proof-of-concept, we implement a new resource configuration mechanism atop the widely used Kubernetes and Linux to demonstrate its potential benefits and shed light on future scheduler redesign. Our proof-of-concept, compared to the existing scheduler, improves the performance of both batch and interactive containerized apps by up to 5.6x and 13.7x.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {243–257},
numpages = {15},
keywords = {Resource management, Cloud, CPU scheduling, Containers, Multi-tenancy},
location = {Seattle, WA, USA},
series = {SoCC '21},
code = {https://github.com/njuliuli/kubernetes/tree/policy},
pdf = {socc21-rkube.pdf}
}

@inproceedings{fedat_sc21,
author = {Chai, Zheng and Chen, Yujing and Anwar, Ali and Zhao, Liang and Cheng, Yue and Rangwala, Huzefa},
title = {FedAT: A High-Performance and Communication-Efficient Federated Learning System with Asynchronous Tiers},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476211},
doi = {10.1145/3458817.3476211},
abstract = {Federated learning (FL) involves training a model over massive distributed devices, while keeping the training data localized and private. This form of collaborative learning exposes new tradeoffs among model convergence speed, model accuracy, balance across clients, and communication cost, with new challenges including: (1) straggler problem---where clients lag due to data or (computing and network) resource heterogeneity, and (2) communication bottleneck---where a large number of clients communicate their local updates to a central server and bottleneck the server. Many existing FL methods focus on optimizing along only one single dimension of the tradeoff space. Existing solutions use asynchronous model updating or tiering-based, synchronous mechanisms to tackle the straggler problem. However, asynchronous methods can easily create a communication bottleneck, while tiering may introduce biases that favor faster tiers with shorter response latencies.To address these issues, we present FedAT, a novel Federated learning system with Asynchronous Tiers under Non-i.i.d. training data. FedAT synergistically combines synchronous, intra-tier training and asynchronous, cross-tier training. By bridging the synchronous and asynchronous training through tiering, FedAT minimizes the straggler effect with improved convergence speed and test accuracy. FedAT uses a straggler-aware, weighted aggregation heuristic to steer and balance the training across clients for further accuracy improvement. FedAT compresses uplink and downlink communications using an efficient, polyline-encoding-based compression algorithm, which minimizes the communication cost. Results show that FedAT improves the prediction performance by up to 21.09% and reduces the communication cost by up to 8.5\texttimes{}, compared to state-of-the-art FL methods.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {60},
numpages = {16},
keywords = {asynchronous distributed learning, tiering, federated learning, weighted aggregation, communication efficiency},
location = {St. Louis, Missouri},
series = {SC '21},
abbr = {SC'21},
pdf = {sc21-fedat.pdf},
arxiv = {https://arxiv.org/abs/2010.05958}
}

@inproceedings {faasnet_atc21,
author = {Ao Wang and Shuai Chang and Huangshi Tian and Hongqi Wang and Haoran Yang and Huiba Li and Rui Du and Yue Cheng},
title = {{FaaSNet}: Scalable and Fast Provisioning of Custom Serverless Container Runtimes at Alibaba Cloud Function Compute},
booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
year = {2021},
isbn = {978-1-939133-23-6},
pages = {443--457},
url = {https://www.usenix.org/conference/atc21/presentation/wang-ao},
publisher = {USENIX Association},
month = {jul},
abbr = {USENIX ATC'21},
selected={true},
slides = {https://www.usenix.org/conference/atc21/presentation/wang-ao},
pdf = {atc21-faasnet.pdf},
code = {https://github.com/ds2-lab/FaaSNet},
}

@inproceedings{DBLP:journals/corr/abs-2112-09335,
  author    = {Hongyi Li and
               Junxiang Wang and
               Yongchao Wang and
               Yue Cheng and
               Liang Zhao},
  title     = {Community-based Layerwise Distributed Training of Graph Convolutional
               Networks},
  journal   = {CoRR},
  volume    = {abs/2112.09335},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.09335},
  eprinttype = {arXiv},
  eprint    = {2112.09335},
  timestamp = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-09335.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  booktitle = {The 13th International OPT Workshop on Optimization for Machine Learning (OPT’21)},
  abbr = {OPT'21},
  arxiv = {https://arxiv.org/abs/2112.09335}
}

@article{ben_master_thesis,
author = {Benjamin Carver},
title = {Wukong: A Fast, Cost-Effective, and Easy-to-Use Serverless DAG Engine},
year = {2021},
html = {https://mars.gmu.edu/xmlui/handle/1920/12093},
abbr = {Thesis},
pdf = {carver-thesis21.pdf}
}

@INPROCEEDINGS{9338293,
  author={Wang, Junxiang and Chai, Zheng and Cheng, Yue and Zhao, Liang},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)}, 
  title={Toward Model Parallelism for Deep Neural Network Based on Gradient-Free ADMM Framework}, 
  year={2020},
  pages={591-600},
  doi={10.1109/ICDM50108.2020.00068},
  abbr = {ICDM '21},
  arxiv = {https://arxiv.org/abs/2009.02868}
}

@inproceedings{10.1145/3419111.3421286,
author = {Carver, Benjamin and Zhang, Jingyuan and Wang, Ao and Anwar, Ali and Wu, Panruo and Cheng, Yue},
title = {Wukong: A Scalable and Locality-Enhanced Framework for Serverless Parallel Computing},
year = {2020},
isbn = {9781450381376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419111.3421286},
doi = {10.1145/3419111.3421286},
abstract = {Executing complex, burst-parallel, directed acyclic graph (DAG) jobs poses a major challenge for serverless execution frameworks, which will need to rapidly scale and schedule tasks at high throughput, while minimizing data movement across tasks. We demonstrate that, for serverless parallel computations, decentralized scheduling enables scheduling to be distributed across Lambda executors that can schedule tasks in parallel, and brings multiple benefits, including enhanced data locality, reduced network I/Os, automatic resource elasticity, and improved cost effectiveness. We describe the implementation and deployment of our new serverless parallel framework, called Wukong, on AWS Lambda. We show that Wukong achieves near-ideal scalability, executes parallel computation jobs up to 68.17X faster, reduces network I/O by multiple orders of magnitude, and achieves 92.96% tenant-side cost savings compared to numpywren.},
booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing},
pages = {1–15},
numpages = {15},
location = {Virtual Event, USA},
series = {SoCC '20},
abbr = {SoCC'20},
selected={true},
code = {https://github.com/ds2-lab/Wukong},
talk = {https://www.youtube.com/watch?v=W0tENnx_58I},
pdf = {socc20-wukong.pdf},
arxiv = {https://arxiv.org/abs/2010.07268}
}

@inproceedings{DBLP:journals/corr/abs-2009-04053,
  author    = {Junxiang Wang and
               Zheng Chai and
               Yue Cheng and
               Liang Zhao},
  title     = {Tunable Subnetwork Splitting for Model-parallelism of Neural Network
               Training},
  journal   = {CoRR},
  volume    = {abs/2009.04053},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.04053},
  eprinttype = {arXiv},
  eprint    = {2009.04053},
  abbr = {OPTML-ICML'20},
  booktitle = {Beyond First Order Methods in ML Systems},
  arxiv = {https://arxiv.org/abs/2009.04053},
  code = {https://github.com/xianggebenben/TSSM},
}

@inproceedings{10.1145/3369583.3392686,
author = {Chai, Zheng and Ali, Ahsan and Zawad, Syed and Truex, Stacey and Anwar, Ali and Baracaldo, Nathalie and Zhou, Yi and Ludwig, Heiko and Yan, Feng and Cheng, Yue},
title = {TiFL: A Tier-Based Federated Learning System},
year = {2020},
isbn = {9781450370523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369583.3392686},
doi = {10.1145/3369583.3392686},
abstract = {Federated Learning (FL) enables learning a shared model acrossmany clients without violating the privacy requirements. One of the key attributes in FL is the heterogeneity that exists in both resource and data due to the differences in computation and communication capacity, as well as the quantity and content of data among different clients. We conduct a case study to show that heterogeneity in resource and data has a significant impact on training time and model accuracy in conventional FL systems. To this end, we propose TiFL, a Tier-based Federated Learning System, which divides clients into tiers based on their training performance and selects clients from the same tier in each training round to mitigate the straggler problem caused by heterogeneity in resource anddata quantity. To further tame the heterogeneity caused by non-IID (Independent and Identical Distribution) data and resources, TiFL employs an adaptive tier selection approach to update the tiering on-the-fly based on the observed training performance and accuracy. We prototype TiFL in a FL testbed following Google's FL architecture and evaluate it using the state-of-the-art FL benchmarks. Experimental evaluation shows that TiFL outperforms the conventional FL in various heterogeneous conditions. With the proposed adaptive tier selection policy, we demonstrate that TiFL achieves much faster training performance while achieving the same or better test accuracy across the board.},
booktitle = {Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {125–136},
numpages = {12},
keywords = {edge computing, data heterogeneity, non-IID, federated learning, resource heterogeneity, stragglers},
location = {Stockholm, Sweden},
series = {HPDC '20},
abbr = {HPDC'20},
selected={true},
pdf = {hpdc20-tifl.pdf},
arxiv = {https://arxiv.org/abs/2001.09249}
}

@inproceedings {infinicache_fast20,
author = {Ao Wang and Jingyuan Zhang and Xiaolong Ma and Ali Anwar and Lukas Rupprecht and Dimitrios Skourtis and Vasily Tarasov and Feng Yan and Yue Cheng},
title = {{InfiniCache}: Exploiting Ephemeral Serverless Functions to Build a {Cost-Effective} Memory Cache},
booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
year = {2020},
isbn = {978-1-939133-12-0},
address = {Santa Clara, CA},
pages = {267--281},
url = {https://www.usenix.org/conference/fast20/presentation/wang-ao},
publisher = {USENIX Association},
month = feb,
abbr = {USENIX FAST'20},
talk = {https://www.usenix.org/conference/fast20/presentation/wang-ao},
website = {https://ds2-lab.github.io/infinicache/},
selected={true},
pdf={fast20-infinicache.pdf},
press = {https://spectrum.ieee.org/pay-cloud-services-data-tool-news},
blog = {https://mikhail.io/2020/03/infinicache-distributed-cache-on-aws-lambda/},
}



@ARTICLE{8631172,

  author={Cameron, Kirk W. and Anwar, Ali and Cheng, Yue and Xu, Li and Li, Bo and Ananth, Uday and Bernard, Jon and Jearls, Chandler and Lux, Thomas and Hong, Yili and Watson, Layne T. and Butt, Ali R.},

  journal={IEEE Transactions on Parallel and Distributed Systems}, 

  title={MOANA: Modeling and Analyzing I/O Variability in Parallel System Experimental Design}, 

  year={2019},

  volume={30},

  number={8},

  pages={1843-1856},

  doi={10.1109/TPDS.2019.2892129},
  abbr={TPDS},
  html={https://ieeexplore.ieee.org/document/8631172}
}

@inproceedings{zhanghyperfaas,
  title={HyperFaaS: A Truly Elastic Serverless Computing Framework},
  author={Zhang, Jingyuan and Wang, Ao and Li, Min and Chen, Yuan and Cheng, Yue},
  booktitle={USENIX Symposium on Networked Systems Design and Implementation},
  abbr={USENIX NSDI'19},
  year={2019},
  poster={nsdi19-hyperfaas.pdf}
}


@INPROCEEDINGS{8955207,

  author={Carver, Benjamin and Zhang, Jingyuan and Wang, Ao and Cheng, Yue},

  booktitle={2019 IEEE/ACM Fourth International Parallel Data Systems Workshop (PDSW)}, 

  title={In Search of a Fast and Efficient Serverless DAG Engine}, 

  year={2019},

  volume={},

  number={},

  pages={1-10},

  doi={10.1109/PDSW49588.2019.00005},
  abbr={PDSW'19},
  arxiv={https://arxiv.org/abs/1910.05896},
  pdf={pdsw19-wukong.pdf},
  code={https://github.com/ds2-lab/Wukong}
}

@INPROCEEDINGS{8814549,

  author={Littley, Michael and Anwar, Ali and Fayyaz, Hannan and Fayyaz, Zeshan and Tarasov, Vasily and Rupprecht, Lukas and Skourtis, Dimitrios and Mohamed, Mohamed and Ludwig, Heiko and Cheng, Yue and Butt, Ali R.},

  booktitle={2019 IEEE 12th International Conference on Cloud Computing (CLOUD)}, 

  title={Bolt: Towards a Scalable Docker Registry via Hyperconvergence}, 

  year={2019},

  volume={},

  number={},

  pages={358-366},

  doi={10.1109/CLOUD.2019.00065},
  pdf={cloud19-bolt.pdf},
  abbr={Cloud'19}
}

@inproceedings {232971,
author = {Zheng Chai and Hannan Fayyaz and Zeshan Fayyaz and Ali Anwar and Yi Zhou and Nathalie Baracaldo and Heiko Ludwig and Yue Cheng},
title = {Towards Taming the Resource and Data Heterogeneity in Federated Learning},
booktitle = {2019 USENIX Conference on Operational Machine Learning (OpML 19)},
year = {2019},
isbn = {978-1-939133-00-7},
address = {Santa Clara, CA},
pages = {19--21},
url = {https://www.usenix.org/conference/opml19/presentation/chai},
publisher = {USENIX Association},
month = may,
abbr={USENIX OpML'19},
pdf={opml19-fl.pdf},
talk={https://www.usenix.org/conference/opml19/presentation/chai}
}

@inproceedings{10.1145/3313808.3313814,
author = {Liu, Li and Wang, Haoliang and Wang, An and Xiao, Mengbai and Cheng, Yue and Chen, Songqing},
title = {VCPU as a Container: Towards Accurate CPU Allocation for VMs},
year = {2019},
isbn = {9781450360203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313808.3313814},
doi = {10.1145/3313808.3313814},
abstract = {With our increasing reliance on cloud computing, accurate resource allocation of virtual machines&nbsp;(or domains) in the cloud have become more and more important. However, the current design of hypervisors (or virtual machine monitors) fails to accurately allocate resources to the domains in the virtualized environment. In this paper, we claim the root cause is that the protection scope is erroneously used as the resource scope for a domain in the current virtualization design. Such design flaw prevents the hypervisor from accurately accounting resource consumption of each domain. In this paper, using virtual CPUs as a container we propose to redefine the resource scope of a domain, so that the new resource scope is aligned with all the CPU consumption incurred by this domain. As a demonstration, we implement a novel system, called VASE (vCPU as a container), on top of the Xen hypervisor. Evaluations on our testbed have shown our proposed approach is effective in accounting system-wide CPU consumption incurred by domains, while introducing negligible overhead to the system.},
booktitle = {Proceedings of the 15th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {193–206},
numpages = {14},
keywords = {CPU Accounting, Scheduling, Virtual I/O, Cloud Computing},
location = {Providence, RI, USA},
series = {VEE 2019},
pdf={vee19-vase.pdf},
abbr = {VEE'19}
}

@article{book_chapter,
  author={Ali R. Butt and Ali Anwar and Yue Cheng},
  title={SDN helps Big Data to optimize Storage},
  journal={Book Chapter, Big Data and Software Defined Networks, Editor: Javid Taheri. IET, ISBN 978-1-78561-304-3.},
  year={2018},
  abbr={Book Chapter}
}

@INPROCEEDINGS{8622518,

  author={Cheng, Yue and Anwar, Ali and Duan, Xuejing},

  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 

  title={Analyzing Alibaba’s Co-located Datacenter Workloads}, 

  year={2018},

  volume={},

  number={},

  pages={292-297},

  doi={10.1109/BigData.2018.8622518},
  pdf={bigdata18-alibaba.pdf},
  abbr={BigData'18}
}

@INPROCEEDINGS{8665756,

  author={Anwar, Ali and Cheng, Yue and Huang, Hai and Han, Jingoo and Sim, Hyogi and Lee, Dongyoon and Douglis, Fred and Butt, Ali R.},

  booktitle={SC18: International Conference for High Performance Computing, Networking, Storage and Analysis}, 

  title={BESPOKV: Application Tailored Scale-Out Key-Value Stores}, 

  year={2018},

  volume={},

  number={},

  pages={14-29},

  doi={10.1109/SC.2018.00005},
  abbr={SC'18},
  pdf={sc18-bespokv.pdf},
  code={https://github.com/ds2-lab/bespokv}
}

@inproceedings{10.1145/3265723.3265742,
author = {Cheng, Yue and Chai, Zheng and Anwar, Ali},
title = {Characterizing Co-Located Datacenter Workloads: An Alibaba Case Study},
year = {2018},
isbn = {9781450360067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265723.3265742},
doi = {10.1145/3265723.3265742},
booktitle = {Proceedings of the 9th Asia-Pacific Workshop on Systems},
articleno = {12},
numpages = {3},
location = {Jeju Island, Republic of Korea},
series = {APSys '18},
abbr={ApSys'18},
arxiv={https://arxiv.org/abs/1808.02919}
}

@inproceedings {210500,
author = {Ali Anwar and Mohamed Mohamed and Vasily Tarasov and Michael Littley and Lukas Rupprecht and Yue Cheng and Nannan Zhao and Dimitrios Skourtis and Amit S. Warke and Heiko Ludwig and Dean Hildebrand and Ali R. Butt},
title = {Improving Docker Registry Design Based on Production Workload Analysis},
booktitle = {16th USENIX Conference on File and Storage Technologies (FAST 18)},
year = {2018},
isbn = {978-1-931971-42-3},
address = {Oakland, CA},
pages = {265--278},
url = {https://www.usenix.org/conference/fast18/presentation/anwar},
publisher = {USENIX Association},
month = feb,
abbr={USENIX FAST'18},
talk={https://www.usenix.org/conference/fast18/presentation/anwar},
selected={true},
pdf={fast18-docker.pdf},
code={https://github.com/ds2-lab/IBM-docker-registry-traces}
}

@INPROCEEDINGS{8425269,

  author={Zhao, Nannan and Anwar, Ali and Cheng, Yue and Salman, Mohammed and Li, Daping and Wan, Jiguang and Xie, Changsheng and He, Xubin and Wang, Feiyi and Butt, Ali},

  booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 

  title={Chameleon: An Adaptive Wear Balancer for Flash Clusters}, 

  year={2018},

  volume={},

  number={},

  pages={1163-1172},

  doi={10.1109/IPDPS.2018.00125},
  pdf={ipdps18-chameleon.pdf},
  abbr={IPDPS'18}
}

@article{yue_phd_dissertation,
author = {Yue Cheng},
title = {Workload-aware Efficient Storage Systems},
year = {2017},
html = {https://vtechworks.lib.vt.edu/handle/10919/78677},
abbr = {Doctoral Dissertation}
}

@ARTICLE{7436639,

  author={Cheng, Yue and Iqbal, M. Safdar and Gupta, Aayush and Butt, Ali R.},

  journal={IEEE Internet Computing}, 

  title={Provider versus Tenant Pricing Games for Hybrid Object Stores in the Cloud}, 

  year={2016},

  volume={20},

  number={3},

  pages={28-35},

  doi={10.1109/MIC.2016.50},
  abbr={Internet Computing},
  html={https://ieeexplore.ieee.org/document/7436639}
}


@inproceedings {196241,
author = {Yue Cheng and Fred Douglis and Philip Shilane and Grant Wallace and Peter Desnoyers and Kai Li},
title = {Erasing {Belady{\textquoteright}s} Limitations: In Search of Flash Cache Offline Optimality},
booktitle = {2016 USENIX Annual Technical Conference (USENIX ATC 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {379--392},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/cheng},
publisher = {USENIX Association},
month = jun,
abbr={USENIX ATC'16},
selected={true},
pdf={atc16-paper-cheng.pdf},
talk={https://www.usenix.org/conference/atc16/technical-sessions/presentation/cheng}
}

@inproceedings {196390,
author = {Ali Anwar and Yue Cheng and Hai Huang and Ali R. Butt},
title = {{ClusterOn}: Building Highly Configurable and Reusable Clustered Data Services Using Simple Data Nodes},
booktitle = {8th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 16)},
year = {2016},
address = {Denver, CO},
url = {https://www.usenix.org/conference/hotstorage16/workshop-program/presentation/anwar},
publisher = {USENIX Association},
month = jun,
abbr = {HotStorage'16},
pdf={hotstorage16-clusteron.pdf},
talk={https://www.usenix.org/conference/hotstorage16/workshop-program/presentation/anwar}
}

@inproceedings{10.1145/2907294.2907304,
author = {Anwar, Ali and Cheng, Yue and Gupta, Aayush and Butt, Ali R.},
title = {MOS: Workload-Aware Elasticity for Cloud Object Stores},
year = {2016},
isbn = {9781450343145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2907294.2907304},
doi = {10.1145/2907294.2907304},
abstract = {The use of cloud object stores has been growing rapidly in recent years as they combine key advantages such as HTTP-based RESTful APIs, high availability, elasticity with a "pay-as-you-go" pricing model that allows applications to scale as needed. The current practice is to either use a single set of configuration parameters or rely on statically configured storage policies for a cloud object store deployment, even when the store is used to support different types of applications with evolving requirements. This crucial mismatch between the different applications requirements and capabilities of the object store is problematic and should be addressed to achieve high efficiency and performance.In this paper, we propose MOS, a Micro Object Storage architecture, which supports independently configured microstores each tuned dynamically to the needs of a particular type of workload. We also design an enhancement, MOS++, that extends MOS's capabilities through fine-grained resource management to effectively meet the tenants' SLAs while maximizing resource efficiency. We have implemented a prototype of MOS ++ in OpenStack Swift using Docker containers. Our evaluation shows that MOS ++ can effectively support heterogeneous workloads across multiple tenants. Compared to default and statically configured object store setups, for a two-tenant setup, MOS++ improves the sustained access bandwidth by up to 79% for a large-object workload, while reducing the 95th percentile latency by up to 70.2% for a small-object workload.},
booktitle = {Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing},
pages = {177–188},
numpages = {12},
keywords = {performance analysis, object store, resource management and scheduling},
location = {Kyoto, Japan},
series = {HPDC '16},
abbr = {HPDC'16},
pdf = {hpdc16-mos.pdf}
}

@INPROCEEDINGS{7529984,

  author={Anwar, Ali and Cheng, Yue and Butt, Ali R.},

  booktitle={2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 

  title={Towards Managing Variability in the Cloud}, 

  year={2016},

  volume={},

  number={},

  pages={1081-1084},

  doi={10.1109/IPDPSW.2016.62},
  pdf = {varsys16.pdf},
  abbr = {VarSys'16}
}

@inproceedings{10.1145/2834976.2834980,
author = {Anwar, Ali and Cheng, Yue and Gupta, Aayush and Butt, Ali R.},
title = {Taming the Cloud Object Storage with MOS},
year = {2015},
isbn = {9781450340083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2834976.2834980},
doi = {10.1145/2834976.2834980},
abstract = {Cloud object stores today are deployed using a single set of configuration parameters for all different types of applications. This homogeneous setup results in all applications experiencing the same service level (e.g., data transfer throughput, etc.). However, the vast variety of applications expose extremely different latency and throughput requirements. To this end, we propose MOS, a <u>M</u>icro <u>O</u>bject <u>S</u>torage architecture with independently configured microstores each tuned dynamically for a particular type of workload. We then expose these microstores to the tenant who can then choose to place their data in the appropriate microstore according the latency and throughput requirements of their workloads. Our evaluation shows that compared with default setup, MOS can improve the performance up to 200% for small objects and 28% for large objects while providing opportunity of tradeoff between two.},
booktitle = {Proceedings of the 10th Parallel Data Storage Workshop},
pages = {7–12},
numpages = {6},
location = {Austin, Texas},
series = {PDSW '15},
pdf={pdsw15-mos.pdf},
abbr={PDSW'15}
}

@inproceedings {190611,
author = {Yue Cheng and M. Safdar Iqbal and Aayush Gupta and Ali R. Butt},
title = {Pricing Games for Hybrid Object Stores in the Cloud: Provider vs. Tenant},
booktitle = {7th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 15)},
year = {2015},
address = {Santa Clara, CA},
url = {https://www.usenix.org/conference/hotcloud15/workshop-program/presentation/cheng},
publisher = {USENIX Association},
month = jul,
abbr={HotCloud'15},
pdf={hotcloud15-pricing.pdf},
talk = {https://www.usenix.org/conference/hotcloud15/workshop-program/presentation/cheng}
}

@inproceedings{10.1145/2749246.2749252,
author = {Cheng, Yue and Iqbal, M. Safdar and Gupta, Aayush and Butt, Ali R.},
title = {CAST: Tiering Storage for Data Analytics in the Cloud},
year = {2015},
isbn = {9781450335508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749246.2749252},
doi = {10.1145/2749246.2749252},
abstract = {Enterprises are increasingly moving their big data analytics to the cloud with the goal of reducing costs without sacrificing application performance. Cloud service providers offer their tenants a myriad of storage options, which while flexible, makes the choice of storage deployment non trivial. Crafting deployment scenarios to leverage these choices in a cost-effective manner - under the unique pricing models and multi-tenancy dynamics of the cloud environment - presents unique challenges in designing cloud-based data analytics frameworks.In this paper, we propose CAST, a Cloud Analytics Storage Tiering solution that cloud tenants can use to reduce monetary cost and improve performance of analytics workloads. The approach takes the first step towards providing storage tiering support for data analytics in the cloud. CAST performs offline workload profiling to construct job performance prediction models on different cloud storage services, and combines these models with workload specifications and high-level tenant goals to generate a cost-effective data placement and storage provisioning plan. Furthermore, we build CAST++ to enhance CAST's optimization model by incorporating data reuse patterns and across-jobs interdependencies common in realistic analytics workloads. Tests with production workload traces from Facebook and a 400-core Google Cloud based Hadoop cluster demonstrate that CAST++ achieves 1.21X performance and reduces deployment costs by 51.4% compared to local storage configuration.},
booktitle = {Proceedings of the 24th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {45–56},
numpages = {12},
keywords = {cloud computing, mapreduce, storage tiering, big data analytics},
location = {Portland, Oregon, USA},
series = {HPDC '15},
abbr = {HPDC'15},
pdf={hpdc15-cast.pdf},
talk = {http://www.hpdc.org/2015/program/slides/cheng.pdf}
}

@inproceedings{10.1145/2741948.2741967,
author = {Cheng, Yue and Gupta, Aayush and Butt, Ali R.},
title = {An In-Memory Object Caching Framework with Adaptive Load Balancing},
year = {2015},
isbn = {9781450332385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2741948.2741967},
doi = {10.1145/2741948.2741967},
abstract = {The extreme latency and throughput requirements of modern web applications are driving the use of distributed in-memory object caches such as Memcached. While extant caching systems scale-out seamlessly, their use in the cloud --- with its unique cost and multi-tenancy dynamics --- presents unique opportunities and design challenges.In this paper, we propose MBal, a high-performance in-memory object caching framework with adaptive <u>M</u>ultiphase load <u>B</u>alancing, which supports not only horizontal (scale-out) but vertical (scale-up) scalability as well. MBal is able to make efficient use of available resources in the cloud through its fine-grained, partitioned, lockless design. This design also lends itself naturally to provide adaptive load balancing both within a server and across the cache cluster through an event-driven, multi-phased load balancer. While individual load balancing approaches are being lever-aged in in-memory caches, MBal goes beyond the extant systems and offers a holistic solution wherein the load balancing model tracks hotspots and applies different strategies based on imbalance severity -- key replication, server-local or cross-server coordinated data migration. Performance evaluation on an 8-core commodity server shows that compared to a state-of-the-art approach, MBal scales with number of cores and executes 2.3x and 12x more queries/second for GET and SET operations, respectively.},
booktitle = {Proceedings of the Tenth European Conference on Computer Systems},
articleno = {4},
numpages = {16},
location = {Bordeaux, France},
series = {EuroSys '15},
abbr={EuroSys'15},
pdf={eurosys15-mbal.pdf},
talk={https://www.youtube.com/watch?v=Ycw5334PCmI}
}

@inproceedings{10.1145/2523616.2525964,
author = {Cheng, Yue and Gupta, Aayush and Povzner, Anna and Butt, Ali R.},
title = {High Performance In-Memory Caching through Flexible Fine-Grained Services},
year = {2013},
isbn = {9781450324281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523616.2525964},
doi = {10.1145/2523616.2525964},
abstract = {In-memory object caches are extensively used in today's web installations [1, 6]. Most existing systems adopt monolithic storage models and engineer optimizations on specific workload characteristics [3, 6] or operations [4, 5]. Such optimizations are insufficient as large-scale cloud workloads typically exhibit both temporal and spatial shifts - requirements vary within the same deployment over time and different parts of the same workload demonstrate different access patterns. To this end, we propose a caching tier that supports differentiated services in multiple dimensions. Since there is no best "one-size-fits-all" solution for all workload requirements, we argue that a fine-grained modular design will provide both high performance and flexibility in supporting multiple services.},
booktitle = {Proceedings of the 4th Annual Symposium on Cloud Computing},
articleno = {56},
numpages = {2},
location = {Santa Clara, California},
series = {SOCC '13},
abbr={SoCC'13},
pdf={socc13-cache.pdf},
poster={socc13-poster.pdf}
}
